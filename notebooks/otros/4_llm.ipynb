{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2bd7a10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 'Ready.')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests, json\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "OLLAMA_URL = \"http://localhost:11434\"\n",
    "OLLAMA_MODEL = \"llama3.2:3b\"\n",
    "\n",
    "prompt = \"Say 'ready' in one word.\"\n",
    "r = requests.post(f\"{OLLAMA_URL}/api/generate\",\n",
    "                  json={\"model\": OLLAMA_MODEL, \"prompt\": prompt, \"stream\": False}, timeout=10)\n",
    "r.status_code, r.json().get(\"response\",\"\")[:80]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b759f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.chdir(os.getcwd().split('\\\\notebooks')[0])\n",
    "main = os.getcwd()\n",
    "\n",
    "DB_PATH = main + \"\\\\data\\\\temporal\\\\exams.db\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fc7a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "OLLAMA_OPTIONS = {\"num_ctx\": 1024, \"num_gpu\": 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ae73cac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema OK\n"
     ]
    }
   ],
   "source": [
    "con = sqlite3.connect(DB_PATH)\n",
    "cols = [r[1] for r in con.execute(\"PRAGMA table_info(attempts);\")]\n",
    "if \"reasons\" not in cols:\n",
    "    con.execute(\"ALTER TABLE attempts ADD COLUMN reasons TEXT;\")\n",
    "    con.commit()\n",
    "con.close()\n",
    "print(\"Schema OK\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46df2a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "con = sqlite3.connect(DB_PATH)\n",
    "cur = con.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "596fddd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema OK (attempts has reasons, hint, feedback_json).\n"
     ]
    }
   ],
   "source": [
    "# add columns if missing\n",
    "cols = [r[1] for r in cur.execute(\"PRAGMA table_info(attempts);\")]\n",
    "if \"reasons\" not in cols:\n",
    "    cur.execute(\"ALTER TABLE attempts ADD COLUMN reasons TEXT;\")\n",
    "if \"hint\" not in cols:\n",
    "    cur.execute(\"ALTER TABLE attempts ADD COLUMN hint TEXT;\")\n",
    "if \"feedback_json\" not in cols:\n",
    "    cur.execute(\"ALTER TABLE attempts ADD COLUMN feedback_json TEXT;\")\n",
    "\n",
    "con.commit(); con.close()\n",
    "print(\"Schema OK (attempts has reasons, hint, feedback_json).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f5a175",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grade_with_local_llm(question:str, solution:str, student:str, timeout=30):\n",
    "    \"\"\"\n",
    "    Calls Ollama locally to grade the student's answer.\n",
    "    Returns a dict like the baseline grader.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"Grade the student's answer concisely.\n",
    "\n",
    "Question: {question}\n",
    "Reference solution: {solution}\n",
    "Student answer: {student}\n",
    "\n",
    "Return a JSON object with:\n",
    "- score: a number between 0 and 1\n",
    "- correct: true/false\n",
    "- reasons: a short string\n",
    "\"\"\"\n",
    "    try:\n",
    "        resp = requests.post(f\"{OLLAMA_URL}/api/generate\",\n",
    "                             json={\"model\": OLLAMA_MODEL, \"prompt\": prompt, \"stream\": False},\n",
    "                             timeout=timeout)\n",
    "        txt = resp.json().get(\"response\",\"\").strip()\n",
    "        data = json.loads(txt) if txt.startswith(\"{\") else None\n",
    "        if not data:\n",
    "            return None\n",
    "        # normalize output\n",
    "        return {\n",
    "            \"score\": float(data.get(\"score\", 0)),\n",
    "            \"correct\": bool(data.get(\"correct\", False)),\n",
    "            \"cosine\": None, \"jaccard\": None, \"missing_keywords\": [],\n",
    "            \"reasons\": data.get(\"reasons\",\"\")\n",
    "        }\n",
    "    except Exception:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2be69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect(): return sqlite3.connect(DB_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a7125ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (paste these small helper stubs or import them if you saved to a .py)\n",
    "def get_user_id(username:str, db_path=DB_PATH) -> int:\n",
    "    con = sqlite3.connect(db_path); cur = con.cursor()\n",
    "    cur.execute(\"INSERT OR IGNORE INTO users(username) VALUES(?)\", (username,))\n",
    "    con.commit()\n",
    "    cur.execute(\"SELECT user_id FROM users WHERE username=?\", (username,))\n",
    "    uid = cur.fetchone()[0]; con.close(); return uid\n",
    "\n",
    "def pick_unseen(username:str, k:int=5, db_path=DB_PATH):\n",
    "    uid = get_user_id(username, db_path)\n",
    "    con = sqlite3.connect(db_path); cur = con.cursor()\n",
    "    cur.execute(\"\"\"\n",
    "      SELECT q.exercise_id, q.topic_pred, e.date\n",
    "      FROM questions q JOIN exams e ON q.exam_id=e.exam_id\n",
    "      WHERE q.exercise_id NOT IN (SELECT exercise_id FROM attempts WHERE user_id=?)\n",
    "      ORDER BY e.date ASC LIMIT ?\"\"\", (uid, k))\n",
    "    rows = cur.fetchall(); con.close()\n",
    "    return rows\n",
    "\n",
    "def fetch_question(exercise_id:str, db_path=DB_PATH):\n",
    "    con = sqlite3.connect(db_path); cur = con.cursor()\n",
    "    cur.execute(\"\"\"\n",
    "      SELECT q.exercise_id, q.question, q.solution, q.topic_pred, e.date, e.exam_type\n",
    "      FROM questions q JOIN exams e ON q.exam_id=e.exam_id\n",
    "      WHERE q.exercise_id=?\"\"\", (exercise_id,))\n",
    "    row = cur.fetchone(); con.close()\n",
    "    return dict(zip([\"exercise_id\",\"question\",\"solution\",\"topic\",\"date\",\"exam_type\"], row))\n",
    "\n",
    "\n",
    "def _clean(s:str) -> str:\n",
    "    s = (s or \"\").lower()\n",
    "    s = re.sub(r\"\\(cid:\\d+\\)\", \" \", s); s = re.sub(r\"[^a-z0-9\\-\\+\\*/\\^\\=\\(\\)\\[\\]\\{\\}\\., ]+\",\" \",s)\n",
    "    return re.sub(r\"\\s+\",\" \", s).strip()\n",
    "STOP = set(\"the a an and or of to for with from in on at is are be was were by as that this these those into over under if then else such\".split())\n",
    "def _keywords(s:str):\n",
    "    toks = re.findall(r\"[a-z0-9\\^\\+\\-\\*/=]+\", _clean(s))\n",
    "    return {t for t in toks if len(t)>=2 and t not in STOP}\n",
    "def grade_answer(solution:str, student:str):\n",
    "    sol = _clean(solution); ans = _clean(student)\n",
    "    vec = TfidfVectorizer(analyzer=\"char_wb\", ngram_range=(3,5), min_df=1)\n",
    "    X = vec.fit_transform([sol, ans]); X = normalize(X)\n",
    "    cos = float((X[0] @ X[1].T).A[0,0]) if X.shape[1] else 0.0\n",
    "    Ks, Ka = _keywords(sol), _keywords(ans)\n",
    "    jac = len(Ks & Ka) / max(1, len(Ks | Ka))\n",
    "    score = 0.6*cos + 0.4*jac\n",
    "    return {\"score\": round(score,4), \"correct\": score>=0.6, \"cosine\": round(cos,4), \"jaccard\": round(jac,4),\n",
    "            \"missing_keywords\": list((Ks-Ka))[:8]}\n",
    "\n",
    "def submit_answer_db(username:str, exercise_id:str, student_answer:str, db_path=DB_PATH):\n",
    "    uid = get_user_id(username, db_path)\n",
    "    q = fetch_question(exercise_id, db_path)\n",
    "    g = grade_answer(q[\"solution\"], student_answer)\n",
    "    con = sqlite3.connect(db_path); cur = con.cursor()\n",
    "    cur.execute(\"\"\"INSERT INTO attempts(user_id, exercise_id, score, correct, cosine, jaccard, missing_keywords, student_answer)\n",
    "                   VALUES(?,?,?,?,?,?,?,?)\"\"\",\n",
    "                (uid, exercise_id, g[\"score\"], int(g[\"correct\"]), g[\"cosine\"], g[\"jaccard\"],\n",
    "                 json.dumps(g[\"missing_keywords\"]), student_answer))\n",
    "    con.commit(); con.close()\n",
    "    return {\"exercise_id\": exercise_id, \"topic\": q[\"topic\"], **g}\n",
    "\n",
    "def save_attempt(uid:int, exercise_id:str, g:dict, student_answer:str):\n",
    "    con = connect(); cur = con.cursor()\n",
    "    cur.execute(\"\"\"\n",
    "      INSERT INTO attempts(user_id, exercise_id, score, correct, cosine, jaccard, missing_keywords, student_answer)\n",
    "      VALUES(?,?,?,?,?,?,?,?)\n",
    "    \"\"\", (uid, exercise_id, g[\"score\"], int(g[\"correct\"]), g[\"cosine\"], g[\"jaccard\"],\n",
    "          json.dumps(g.get(\"missing_keywords\",[])), student_answer))\n",
    "    con.commit(); con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4c1abadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def submit_answer_llm_first(username:str, exercise_id:str, student_answer:str):\n",
    "    q = fetch_question(exercise_id)  # your existing helper (DB-backed)\n",
    "    # Try LLM\n",
    "    g = grade_with_local_llm(q[\"question\"], q[\"solution\"], student_answer)\n",
    "    # Fallback to baseline if LLM failed\n",
    "    if not g:\n",
    "        g = grade_answer(q[\"solution\"], student_answer)  # your baseline grader\n",
    "    # Save\n",
    "    _uid = get_user_id(username)\n",
    "    save_attempt(_uid, exercise_id, g, student_answer)\n",
    "    return g\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a3cb8f87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.0,\n",
       " 'correct': False,\n",
       " 'cosine': None,\n",
       " 'jaccard': None,\n",
       " 'missing_keywords': [],\n",
       " 'reasons': \"The student's response appears to be unrelated to the Riesz representation theorem. They mentioned contraction mapping, which is a concept from metric spaces and has no direct connection to the characterization of linear functionals.\"}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = submit_answer_llm_first(\"student1\", \"Exercise 1\", \"My attempt about contraction mapping \")\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6ee343",
   "metadata": {},
   "source": [
    "# LLM FEEDBACK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e92d2294",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, json, re\n",
    "\n",
    "def llm_grade_and_feedback(question:str, solution:str, student:str, timeout=60):\n",
    "    \"\"\"\n",
    "    Ask the local LLM for a compact judgment + 1-line hint.\n",
    "    Returns dict or None on failure.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"You grade short math answers. Be brief and avoid giving the full solution.\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Reference solution:\n",
    "{solution}\n",
    "\n",
    "Student answer:\n",
    "{student}\n",
    "\n",
    "Return ONLY a compact JSON with exactly these keys:\n",
    "- \"score\": number 0..1\n",
    "- \"correct\": true/false\n",
    "- \"explanation\": one short sentence explaining the verdict\n",
    "- \"hint\": one short hint the student can try next (DO NOT reveal the full solution)\n",
    "\"\"\"\n",
    "    try:\n",
    "        r = requests.post(f\"{OLLAMA_URL}/api/generate\",\n",
    "                          json={\"model\": OLLAMA_MODEL, \"prompt\": prompt,\n",
    "                                \"stream\": False, \"format\":\"json\", \"options\": OLLAMA_OPTIONS},\n",
    "                          timeout=timeout)\n",
    "        data = json.loads(r.json().get(\"response\",\"\").strip())\n",
    "        # normalize\n",
    "        return {\n",
    "            \"score\": float(data.get(\"score\", 0)),\n",
    "            \"correct\": bool(data.get(\"correct\", False)),\n",
    "            \"reasons\": data.get(\"explanation\",\"\"),\n",
    "            \"hint\": data.get(\"hint\",\"\"),\n",
    "            \"cosine\": None, \"jaccard\": None, \"missing_keywords\": []\n",
    "        }\n",
    "    except Exception:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9036c2b",
   "metadata": {},
   "source": [
    "# SAVE ATTEMPT WITH FEEDBACK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "aaa6bc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grade_best_with_feedback(question:str, solution:str, student:str):\n",
    "    g = llm_grade_and_feedback(question, solution, student)\n",
    "    if g: \n",
    "        return g\n",
    "    # fallback to baseline if LLM fails\n",
    "    b = grade_answer(solution, student)  # <- your existing baseline function\n",
    "    b[\"reasons\"] = \"Similarity-based baseline verdict.\"\n",
    "    b[\"hint\"] = \"\"\n",
    "    return b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "afd3d2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3, json\n",
    "\n",
    "def save_attempt(uid:int, exercise_id:str, g:dict, student_answer:str, db_path=DB_PATH):\n",
    "    con = sqlite3.connect(db_path); cur = con.cursor()\n",
    "    cur.execute(\"\"\"\n",
    "      INSERT INTO attempts(user_id, exercise_id, score, correct, cosine, jaccard,\n",
    "                           missing_keywords, student_answer, reasons, hint, feedback_json)\n",
    "      VALUES(?,?,?,?,?,?,?,?,?,?,?)\n",
    "    \"\"\", (uid, exercise_id,\n",
    "          g.get(\"score\",0.0), int(bool(g.get(\"correct\",False))),\n",
    "          g.get(\"cosine\"), g.get(\"jaccard\"),\n",
    "          json.dumps(g.get(\"missing_keywords\",[])),\n",
    "          student_answer, g.get(\"reasons\",\"\"), g.get(\"hint\",\"\"),\n",
    "          json.dumps({k:v for k,v in g.items() if k not in (\"cosine\",\"jaccard\",\"missing_keywords\")})\n",
    "         ))\n",
    "    con.commit(); con.close()\n",
    "\n",
    "def submit_answer_with_feedback(username:str, exercise_id:str, student_answer:str):\n",
    "    uid = get_user_id(username)\n",
    "    q = fetch_question(exercise_id)\n",
    "    g = grade_best_with_feedback(q[\"question\"], q[\"solution\"], student_answer)\n",
    "    save_attempt(uid, exercise_id, g, student_answer)\n",
    "    return {\"exercise_id\": exercise_id, \"topic\": q[\"topic\"], \"date\": q[\"date\"], **g}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8e3c4b",
   "metadata": {},
   "source": [
    "# Next Question (Recommender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e6beab57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_next(username:str, k:int=5, db_path=DB_PATH):\n",
    "    uid = get_user_id(username, db_path)\n",
    "    con = sqlite3.connect(db_path)\n",
    "\n",
    "    # unseen pool\n",
    "    unseen = pd.read_sql(\"\"\"\n",
    "      SELECT q.exercise_id, q.topic_pred AS topic\n",
    "      FROM questions q\n",
    "      WHERE q.exercise_id NOT IN (SELECT exercise_id FROM attempts WHERE user_id=?)\n",
    "    \"\"\", con, params=(uid,))\n",
    "\n",
    "    # recent mistakes to review (last attempt was incorrect)\n",
    "    mistakes = pd.read_sql(\"\"\"\n",
    "      SELECT a.exercise_id, q.topic_pred AS topic, MAX(a.ts) as last_ts\n",
    "      FROM attempts a JOIN questions q ON a.exercise_id=q.exercise_id\n",
    "      WHERE a.user_id=? AND a.correct=0\n",
    "      GROUP BY a.exercise_id, q.topic_pred\n",
    "      ORDER BY last_ts DESC\n",
    "      LIMIT 10\n",
    "    \"\"\", con, params=(uid,))\n",
    "\n",
    "    # weak topics by avg score\n",
    "    perf = pd.read_sql(\"\"\"\n",
    "      SELECT q.topic_pred AS topic, AVG(a.score) AS avg_score\n",
    "      FROM attempts a JOIN questions q ON a.exercise_id=q.exercise_id\n",
    "      WHERE a.user_id=?\n",
    "      GROUP BY q.topic_pred\n",
    "      ORDER BY avg_score ASC\n",
    "    \"\"\", con, params=(uid,))\n",
    "    con.close()\n",
    "\n",
    "    picks = []\n",
    "\n",
    "    # 1) take up to ceil(k*0.4) recent mistakes to review\n",
    "    n_rev = max(1, int(0.4*k))\n",
    "    if not mistakes.empty:\n",
    "        picks += mistakes[\"exercise_id\"].tolist()[:n_rev]\n",
    "\n",
    "    # 2) fill remaining with unseen, prioritizing weak topics\n",
    "    remaining = k - len(picks)\n",
    "    if remaining > 0 and not unseen.empty:\n",
    "        weak = set(perf[\"topic\"].head(max(1, len(perf)//2)).tolist()) if not perf.empty else set()\n",
    "        unseen_weak = unseen[unseen[\"topic\"].isin(weak)]\n",
    "        pool = unseen_weak if not unseen_weak.empty else unseen\n",
    "        picks += pool.sample(min(remaining, len(pool)), random_state=42)[\"exercise_id\"].tolist()\n",
    "\n",
    "    # de-dup just in case\n",
    "    seen = set(); ordered=[]\n",
    "    for x in picks:\n",
    "        if x not in seen:\n",
    "            ordered.append(x); seen.add(x)\n",
    "    return ordered[:k]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbe7eb9",
   "metadata": {},
   "source": [
    "# Quick test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "15b21d72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Exercise 3 | Topic: brouwer_fixed_point_theorem | Date: 2025-08-29\n",
      "Prove that a linear operator T : V V between normed vector spaces is continuous at 1 2 ! a point v V if and only if it is continuous on V . 1 1 2 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "USERNAME = \"student1\"\n",
    "\n",
    "# pick any unseen question (or choose one ID manually)\n",
    "rows = pick_unseen(USERNAME, k=1)\n",
    "if not rows:\n",
    "    print(\"No unseen questions left – try resetting attempts or loading more data.\")\n",
    "else:\n",
    "    qid = rows[0][0]\n",
    "    q = fetch_question(qid)\n",
    "    print(\"Question:\", q[\"exercise_id\"], \"| Topic:\", q[\"topic\"], \"| Date:\", q[\"date\"])\n",
    "    print(q[\"question\"], \"\\n\")\n",
    "\n",
    "    # simulate an answer:\n",
    "    res = submit_answer_llm_first(USERNAME, qid, \"Outline the contraction mapping argument and fixed point uniqueness...\")\n",
    "    res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9b0749c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommendations: ['Exercise 1', 'Exercise 4']\n",
      "\n",
      "Question Exercise 1 · linear_functionals_and_operators · 2025-08-29\n",
      "Consider a functional f : R n R. State and prove the Riesz representation theorem ! (that is, the theorem that provides a characterization for linear functionals).\n",
      "\n",
      "Verdict: ❌ | Score: 0.2\n",
      "Why: Incorrect approach\n",
      "Hint: Focus on Riesz's original paper for inspiration\n"
     ]
    }
   ],
   "source": [
    "USERNAME = \"student1\"\n",
    "\n",
    "# pick or recommend\n",
    "cand = recommend_next(USERNAME, k=3)\n",
    "print(\"Recommendations:\", cand)\n",
    "\n",
    "if not cand:\n",
    "    # fall back to any unseen\n",
    "    rows = pick_unseen(USERNAME, k=1)\n",
    "    cand = [rows[0][0]] if rows else []\n",
    "\n",
    "if cand:\n",
    "    ex_id = cand[0]\n",
    "    q = fetch_question(ex_id)\n",
    "    print(f\"\\nQuestion {q['exercise_id']} · {q['topic']} · {q['date']}\\n{q['question']}\\n\")\n",
    "    # simulate student answer\n",
    "    res = submit_answer_with_feedback(USERNAME, ex_id, \"I think we use a contraction and apply Banach to get the fixed point...\")\n",
    "    print(\"Verdict:\", \"✅\" if res[\"correct\"] else \"❌\", \"| Score:\", res[\"score\"])\n",
    "    print(\"Why:\", res[\"reasons\"])\n",
    "    print(\"Hint:\", res[\"hint\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
