{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 2 — Baseline grader + Golden set evaluation + pick threshold\n",
    "\n",
    "**Goal:** implement a simple baseline grader, evaluate on `data/golden.csv`, and choose `GRADE_THRESHOLD`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c1e2944",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(os.getcwd().split('\\\\notebooks')[0])\n",
    "main = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Setup\n",
    "import re, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, classification_report\n",
    "\n",
    "DB_PATH = main + \"\\\\data\\\\temporal\\\\exams.db\"\n",
    "GOLDEN_PATH = main + '\\\\data\\\\golden\\\\golden.csv'\n",
    "import sqlite3\n",
    "def qsol(ex_id):\n",
    "    con = sqlite3.connect(DB_PATH); con.row_factory = sqlite3.Row\n",
    "    r = con.execute(\"SELECT solution FROM questions WHERE exercise_id=?\", (ex_id,)).fetchone()\n",
    "    con.close()\n",
    "    return r[\"solution\"] if r else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Baseline grader (simple, transparent, fast)\n",
    "_word = re.compile(r\"[a-zA-Z]+\")\n",
    "def norm(s): \n",
    "    return \" \".join(_word.findall((s or \"\").lower()))\n",
    "\n",
    "def cosine_tfidf(a, b):\n",
    "    vec = TfidfVectorizer(min_df=1, max_df=0.95, ngram_range=(1,2))\n",
    "    X = vec.fit_transform([norm(a), norm(b)]).toarray()\n",
    "    num = (X[0]*X[1]).sum()\n",
    "    den = (np.linalg.norm(X[0])*np.linalg.norm(X[1]) + 1e-9)\n",
    "    return float(num/den)\n",
    "\n",
    "def jaccard(a, b):\n",
    "    A, B = set(norm(a).split()), set(norm(b).split())\n",
    "    if not A and not B: return 0.0\n",
    "    return float(len(A&B) / max(1, len(A|B)))\n",
    "\n",
    "STOP = set([\"the\",\"and\",\"or\",\"to\",\"of\",\"in\",\"on\",\"for\",\"with\",\"a\",\"an\",\"is\",\"are\",\"be\",\"that\",\"this\"])\n",
    "def keyword_overlap(solution, student, k=8):\n",
    "    toks = [t for t in norm(solution).split() if t not in STOP and len(t)>=4]\n",
    "    if not toks: return 0.0\n",
    "    from collections import Counter\n",
    "    top = [w for w,_ in Counter(toks).most_common(k)]\n",
    "    st = set(norm(student).split())\n",
    "    return float(len([w for w in top if w in st]) / max(1,len(top)))\n",
    "\n",
    "def baseline_grade(solution, student):\n",
    "    c = cosine_tfidf(solution, student)\n",
    "    j = jaccard(solution, student)\n",
    "    k = keyword_overlap(solution, student, k=8)\n",
    "    score = 0.6*c + 0.25*j + 0.15*k\n",
    "    reasons = f\"Cosine={c:.2f}, Jaccard={j:.2f}, Keywords={k:.2f}\"\n",
    "    hint = \"Revisa los pasos clave y la definición usada.\"\n",
    "    return {\"score\": float(score), \"reasons\": reasons, \"hint\": hint}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Load golden set, attach solutions\n",
    "assert GOLDEN_PATH.exists(), \"Put your golden.csv in data/golden.csv\"\n",
    "df = pd.read_csv(GOLDEN_PATH)\n",
    "df[\"solution\"] = df[\"exercise_id\"].map(qsol)\n",
    "df = df.dropna(subset=[\"solution\"]).reset_index(drop=True)\n",
    "label_map = {\"correct\":1, \"partial\":0, \"incorrect\":0}\n",
    "df[\"y_true\"] = df[\"label\"].map(label_map).astype(int)\n",
    "print(\"Golden rows ready:\", len(df))\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Score with baseline\n",
    "df[\"score\"] = [baseline_grade(s, a)[\"score\"] for s,a in zip(df[\"solution\"], df[\"student_answer\"])]\n",
    "df.groupby(\"label\")[\"score\"].mean().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Threshold sweep → pick best F1\n",
    "THRS = np.round(np.linspace(0.30, 0.85, 23), 2)\n",
    "def eval_thr(t):\n",
    "    y_pred = (df[\"score\"].values >= t).astype(int)\n",
    "    acc = accuracy_score(df[\"y_true\"], y_pred)\n",
    "    pr,rc,f1,_ = precision_recall_fscore_support(df[\"y_true\"], y_pred, average=\"binary\", zero_division=0)\n",
    "    return dict(t=t, acc=acc, prec=pr, rec=rc, f1=f1)\n",
    "\n",
    "sweep = pd.DataFrame([eval_thr(t) for t in THRS]).sort_values(\"f1\", ascending=False)\n",
    "best = sweep.iloc[0]\n",
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) Report @ best threshold\n",
    "t = float(best[\"t\"])\n",
    "pred = (df[\"score\"] >= t).astype(int)\n",
    "print(\"Best threshold:\", t)\n",
    "print(classification_report(df[\"y_true\"], pred, target_names=[\"not-correct\",\"correct\"], zero_division=0))\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(\"Confusion matrix:\\n\", confusion_matrix(df[\"y_true\"], pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7) Save suggestion for .env (you'll add it later in app)\n",
    "print(\">>> Suggested GRADE_THRESHOLD =\", round(t, 2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
